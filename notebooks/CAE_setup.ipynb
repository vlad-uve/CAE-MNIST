{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlad-uve/CAE-MNIST/blob/main/notebooks/CAE_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ub956SEtIlI",
        "outputId": "e660579b-bd99-4071-b126-f8ad57d38124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (116 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[01;34m../CAE_setup_local\u001b[0m\n",
            "├── \u001b[01;34mnotebooks\u001b[0m\n",
            "├── \u001b[01;34moutputs\u001b[0m\n",
            "└── \u001b[01;34msrc\u001b[0m\n",
            "\n",
            "3 directories, 0 files\n"
          ]
        }
      ],
      "source": [
        "# Create project folder structure\n",
        "!mkdir -p ../CAE_setup_local/src\n",
        "!mkdir -p ../CAE_setup_local/notebooks\n",
        "!mkdir -p ../CAE_setup_local/outputs\n",
        "\n",
        "# Check structure (install tree if needed)\n",
        "!apt-get install tree -y\n",
        "!tree ../CAE_setup_local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VXIpDzNmb8L"
      },
      "source": [
        "# Model Classes Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q9x4o2T8BrV",
        "outputId": "0215a7c3-efe2-406f-def6-f244fb8a0917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ../CAE_setup_local/src/model.py\n"
          ]
        }
      ],
      "source": [
        "# write encoder, decoder and autoencoder classes to src as model.py\n",
        "%%writefile ../CAE_setup_local/src/model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_channels, stride, padding, latent_dim, use_batch_norm, activation_func):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # store activation function choice\n",
        "        self.activation_func = activation_func\n",
        "\n",
        "        # define convolutional layers for encoding\n",
        "        self.encn1 = nn.Conv2d(1, n_channels[0], 4, stride=stride, padding=padding)\n",
        "        self.encn2 = nn.Conv2d(n_channels[0], n_channels[1], 4, stride=stride, padding=padding)\n",
        "        self.encn3 = nn.Conv2d(n_channels[1], n_channels[2], 3, stride=stride, padding=padding)\n",
        "\n",
        "        # optional batch normalization layers after each conv\n",
        "        if use_batch_norm:\n",
        "            self.bn1 = nn.BatchNorm2d(n_channels[0])\n",
        "            self.bn2 = nn.BatchNorm2d(n_channels[1])\n",
        "            self.bn3 = nn.BatchNorm2d(n_channels[2])\n",
        "        else:\n",
        "            self.bn1 = self.bn2 = self.bn3 = nn.Identity()\n",
        "\n",
        "        # flatten and fully connected bottleneck layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(n_channels[2] * 4 * 4, latent_dim)\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        # safely apply activation function\n",
        "        if self.activation_func == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation_func == 'leaky_relu':\n",
        "            return F.leaky_relu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_func}\")\n",
        "\n",
        "    def forward(self, input_x):\n",
        "        # pass through encoding layers with batchnorm and activation\n",
        "        x = self.encn1(input_x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.apply_activation(x)\n",
        "\n",
        "        x = self.encn2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.apply_activation(x)\n",
        "\n",
        "        x = self.encn3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.apply_activation(x)\n",
        "\n",
        "        # return data encoded in latent space with flattening and fully connected layer\n",
        "        x = self.flatten(x)\n",
        "        encoded_x = self.fc1(x)\n",
        "\n",
        "        return encoded_x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_channels, stride, padding, latent_dim, use_batch_norm, activation_func):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # store activation function choice\n",
        "        self.activation_func = activation_func\n",
        "\n",
        "        # fully connected + unflatten to prepare for decoding\n",
        "        self.fc1 = nn.Linear(latent_dim, n_channels[2] * 4 * 4)\n",
        "        self.unflatten = nn.Unflatten(1, (n_channels[2], 4, 4))\n",
        "\n",
        "        # optional batch normalization layers after each transposed conv\n",
        "        if use_batch_norm:\n",
        "            self.bn1 = nn.BatchNorm2d(n_channels[1])\n",
        "            self.bn2 = nn.BatchNorm2d(n_channels[0])\n",
        "        else:\n",
        "            self.bn1 = self.bn2 = nn.Identity()\n",
        "\n",
        "        # define transposed conv layers for decoding\n",
        "        self.decn1 = nn.ConvTranspose2d(n_channels[2], n_channels[1], 3, stride=stride, padding=padding)\n",
        "        self.decn2 = nn.ConvTranspose2d(n_channels[1], n_channels[0], 4, stride=stride, padding=padding)\n",
        "        self.decn3 = nn.ConvTranspose2d(n_channels[0], 1, 4, stride=stride, padding=padding)\n",
        "\n",
        "    def apply_activation(self, x):\n",
        "        # safely apply activation function\n",
        "        if self.activation_func == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation_func == 'leaky_relu':\n",
        "            return F.leaky_relu(x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation function: {self.activation_func}\")\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        #decoding data from latent space with unflattening of fully connected layer\n",
        "        x = self.fc1(encoded)\n",
        "        x = self.unflatten(x)\n",
        "\n",
        "        # pass through transposed conv layers with batchnorm and activation\n",
        "        x = self.decn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.apply_activation(x)\n",
        "\n",
        "        x = self.decn2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.apply_activation(x)\n",
        "\n",
        "        x = self.decn3(x)\n",
        "        decoded_x = F.sigmoid(x)\n",
        "\n",
        "        # return decoded data as an image with pixels of [0, 1] range\n",
        "        return decoded_x\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, n_channels, latent_dim, use_batch_norm=False, activation_func='relu'):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "\n",
        "        # fixed parameters for all conv layers\n",
        "        stride = 2\n",
        "        padding = 1\n",
        "\n",
        "        # define encoder and decoder blocks\n",
        "        self.encoder = Encoder(n_channels, stride, padding, latent_dim, use_batch_norm, activation_func)\n",
        "        self.decoder = Decoder(n_channels, stride, padding, latent_dim, use_batch_norm, activation_func)\n",
        "\n",
        "    def forward(self, input_x):\n",
        "        # encode and decode the input data\n",
        "        encoded_x = self.encoder(input_x)\n",
        "        decoded_x = self.decoder(encoded_x)\n",
        "\n",
        "        return decoded_x, encoded_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p8_0sl_Gt8h"
      },
      "source": [
        "# Model Training Functions Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkuF_m9ZGx0l",
        "outputId": "d620002d-11d8-42fa-83f1-5a423303d4c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ../CAE_setup_local/src/train.py\n"
          ]
        }
      ],
      "source": [
        "# write model training, model validation, and full model traning run functions to src as train.py\n",
        "%%writefile ../CAE_setup_local/src/train.py\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_model(model, train_dataloader, optimizer, epoch, device):\n",
        "    \"\"\"\n",
        "    Runs one training epoch for the given model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): the autoencoder to train\n",
        "        train_dataloader (DataLoader): training data loader\n",
        "        optimizer (torch.optim.Optimizer): optimizer used for training\n",
        "        epoch (int): current epoch number (used for tracking/logging)\n",
        "\n",
        "    Returns:\n",
        "        float: loss value from the last batch of the epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    for b_i, (input_x, _) in enumerate(train_dataloader):\n",
        "        # move batch to device\n",
        "        input_x = input_x.to(device)\n",
        "\n",
        "        # clear previous gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass: get model output\n",
        "        decoded_x, encoded_x = model(input_x)\n",
        "\n",
        "        # compute reconstruction loss between input and output\n",
        "        loss = F.binary_cross_entropy(decoded_x, input_x)\n",
        "\n",
        "        # backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # return last batch loss\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validate_model(model, validation_dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the validation set using binary cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): trained autoencoder\n",
        "        validation_dataloader (DataLoader): validation data loader\n",
        "        device (str): 'cuda' or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        float: average loss over the entire validation set\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to evaluation mode (disables dropout, batchnorm updates etc.)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    # disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        for input_x, _ in validation_dataloader:\n",
        "            # move batch to device\n",
        "            input_x = input_x.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            decoded_x, encoded_x = model(input_x)\n",
        "\n",
        "            # accumulate reconstruction loss for each bath\n",
        "            total_loss += F.binary_cross_entropy(decoded_x, input_x)\n",
        "\n",
        "    # compute and return average loss over validation over one epoch\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "\n",
        "    return avg_loss.item()\n",
        "\n",
        "\n",
        "def run_model_training(model, train_dataloader, validation_dataloader, optimizer, scheduler, num_epoch, device):\n",
        "    \"\"\"\n",
        "    Trains the model across multiple epochs and evaluates on validation set.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): the autoencoder\n",
        "        train_dataloader (DataLoader): training data\n",
        "        validation_dataloader (DataLoader): validation data\n",
        "        optimizer (Optimizer): optimizer for training\n",
        "        scheduler (LRScheduler): learning rate scheduler\n",
        "        num_epoch (int): number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        model: trained model\n",
        "        dict: loss history containing 'train', 'validation', and 'epoch' lists\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize loss tracking dictionary\n",
        "    loss_history = {'train': [], 'validation': [], 'epoch': []}\n",
        "\n",
        "    print('\\nTRAINING IS STARTED:')\n",
        "\n",
        "    # run training loop\n",
        "    for epoch in range(1, num_epoch + 1):\n",
        "        # train model on training set\n",
        "        train_loss = train_model(model, train_dataloader, optimizer, epoch, device)\n",
        "\n",
        "        # evaluate model on validation set\n",
        "        validation_loss = validate_model(model, validation_dataloader, device)\n",
        "\n",
        "        # check if scheduler reduces learning rate based on validation loss plateau\n",
        "        previous_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(validation_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        if current_lr != previous_lr:\n",
        "            print(f\"LR reduced from {previous_lr:.4f} → {current_lr:.4f}\")\n",
        "\n",
        "        # record losses and epoch number\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['validation'].append(validation_loss)\n",
        "        loss_history['epoch'].append(epoch)\n",
        "\n",
        "        # print progress\n",
        "        print(f\"Epoch {epoch:2d} | Train Loss: {train_loss:.4f} | Validation Loss: {validation_loss:.4f}\")\n",
        "\n",
        "    print('\\nTRAINING IS FINISHED.')\n",
        "\n",
        "    return model, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3XLxXYlmZPH"
      },
      "source": [
        "# Data Loaders Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7xo4noyD1pt",
        "outputId": "b8eb77d0-7f5a-4da4-f7b1-de253cc2565b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ../CAE_setup_local/src/data.py\n"
          ]
        }
      ],
      "source": [
        "# write train and validation dataloader functions to src as data.py\n",
        "\n",
        "%%writefile ../CAE_setup_local/src/data.py\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_train_dataloader(batch_size=32, data_dir='../data'):\n",
        "    \"\"\"\n",
        "    Loads the MNIST training set and returns a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): size of each training batch\n",
        "        data_dir (str): path to MNIST data storage\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: PyTorch DataLoader for training data\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root=data_dir,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def get_validation_dataloader(batch_size=500, data_dir='../data'):\n",
        "    \"\"\"\n",
        "    Loads the MNIST validation (test) set and returns a DataLoader.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): size of each validation batch\n",
        "        data_dir (str): path to MNIST data storage\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: PyTorch DataLoader for validation data\n",
        "    \"\"\"\n",
        "    validation_dataset = datasets.MNIST(\n",
        "        root=data_dir,\n",
        "        train=False,\n",
        "        transform=transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "        validation_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return validation_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWq1fORex3Cb"
      },
      "source": [
        "# Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0sFBX7x-GO",
        "outputId": "aae78197-e0cb-4c93-da17-1650b3d9d26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ../CAE_setup_local/src/evaluation.py\n"
          ]
        }
      ],
      "source": [
        "# write sampling and experiment reconstruction functions to src as evaluation.py\n",
        "\n",
        "%%writefile ../CAE_setup_local/src/evaluation.py\n",
        "\n",
        "import torch\n",
        "\n",
        "def get_image_samples(validation_dataloader):\n",
        "\n",
        "  # get a batch of images and labels from the validation set\n",
        "  images, labels = next(iter(validation_dataloader))\n",
        "\n",
        "  # select exactly one example for each digit (0-9)\n",
        "  sample_labels = [i for i in range(10)]\n",
        "  sample_indices = [torch.where(labels == i)[0][0].item() for i in range(10)]\n",
        "\n",
        "  sample_images = images[sample_indices]\n",
        "\n",
        "  return sample_images, sample_labels\n",
        "\n",
        "\n",
        "\n",
        "def get_experiment_reconstructions(model_list, original_images, device):\n",
        "    '''\n",
        "    Run models on input images and return reconstructed outputs.\n",
        "\n",
        "    Args:\n",
        "        model_list (list): list of trained models to evaluate\n",
        "        original_images (torch.Tensor): batch of original input images\n",
        "\n",
        "    Returns:\n",
        "        list of torch.Tensor: reconstructed images for each model\n",
        "    '''\n",
        "    reconstructions = []\n",
        "    for model in model_list:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            reconstructed_images, _ = model(original_images.to(device))\n",
        "            reconstructions.append(reconstructed_images.cpu())\n",
        "    return reconstructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1znAgpg8eaz"
      },
      "source": [
        "# Visualization Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZinR8SKQfAq",
        "outputId": "0ea79dff-fc18-41f0-ee5f-c3524a9b3b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ../CAE_setup_local/src/plotting.py\n"
          ]
        }
      ],
      "source": [
        "# write visulization helper function to src as plotting.py\n",
        "%%writefile ../CAE_setup_local/src/plotting.py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot_baseline_history(baseline_loss, to_plot_train=False):\n",
        "    '''\n",
        "    Plot loss history for the baseline model.\n",
        "\n",
        "    Args:\n",
        "        baseline_loss (dict): dictionary with 'epoch', 'train', and 'validation' lists\n",
        "        to_plot_train (bool): if True, also plot training loss\n",
        "    '''\n",
        "\n",
        "    color=plt.get_cmap('tab10').colors\n",
        "\n",
        "    # optionally plot training losses\n",
        "    if to_plot_train:\n",
        "        plt.plot(baseline_loss['epoch'], baseline_loss['train'], label='Base model (training loss)', color=color[0], linestyle='--')\n",
        "\n",
        "    # plot validation losses\n",
        "    plt.plot(baseline_loss['epoch'], baseline_loss['validation'], label='Base model (validation loss)', color=color[0], linewidth=2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Baseline Model Loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_experiment_history(loss_list, label_list, title, to_plot_train=False):\n",
        "    '''\n",
        "    Plot loss curves for multiple models.\n",
        "\n",
        "    Args:\n",
        "        loss_list (list of dict): list of loss history dictionaries (per model)\n",
        "        label_list (list of str): list of model names (same length as loss_list)\n",
        "        title (str): title for the plot\n",
        "        to_plot_train (bool): if True, also plot training loss curves\n",
        "\n",
        "    Each dictionary in loss_list must contain:\n",
        "        - 'epoch': list of epoch numbers\n",
        "        - 'train': list of training losses (optional if to_plot_train=False)\n",
        "        - 'validation': list of validation losses\n",
        "    '''\n",
        "\n",
        "    color=plt.get_cmap('tab10').colors\n",
        "\n",
        "    # loop over each loss history in the list\n",
        "    for i, (loss_history, label) in enumerate(zip(loss_list, label_list)):\n",
        "        # optionally plot training losses\n",
        "        if to_plot_train:\n",
        "            plt.plot(loss_history['epoch'], loss_history['train'], label=label + ' (training loss)', color=color[i+1], linestyle='--')\n",
        "\n",
        "        # plot validation losses\n",
        "        plt.plot(loss_history['epoch'], loss_history['validation'], label=label + ' (validation loss)', color=color[i+1], linewidth=2)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_digits_row(images, labels=None, title=None, cmap='magma', figsize=(15, 3)):\n",
        "    '''\n",
        "    Display a row of digit images side by side.\n",
        "\n",
        "    Args:\n",
        "        images (numpy array or torch tensor): array of images (n_images, height, width)\n",
        "        labels (list or array, optional): optional list of labels to display as titles\n",
        "        title (str, optional): overall title for the plot\n",
        "        cmap (str): matplotlib colormap for image display\n",
        "        figsize (tuple): figure size for the plot\n",
        "    '''\n",
        "\n",
        "    n_images = images.shape[0]\n",
        "\n",
        "    fig, axes = plt.subplots(1, n_images, figsize=figsize)\n",
        "\n",
        "    for idx, ax in enumerate(axes.flat):\n",
        "        # display each image\n",
        "        ax.imshow(images[idx], cmap=cmap)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # optionally set image label as title\n",
        "        if labels is not None:\n",
        "            ax.set_title(str(labels[idx]), fontsize=20)\n",
        "\n",
        "    # optionally set a main title for the plot\n",
        "    if title is not None:\n",
        "        plt.suptitle(title, y=1, fontsize=30)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # separator\n",
        "    print('\\n ')\n",
        "\n",
        "\n",
        "def plot_experiment_reconstructions(reconstructions, labels, title_list):\n",
        "    '''\n",
        "    Plot reconstruction results for multiple models.\n",
        "\n",
        "    Args:\n",
        "        reconstructions (list of torch.Tensor): reconstructed outputs from models\n",
        "        labels (list or array): labels for each image\n",
        "        title_list (list of str): titles to display for each model\n",
        "    '''\n",
        "    for recon, title in zip(reconstructions, title_list):\n",
        "        plot_digits_row(\n",
        "            recon.squeeze(),\n",
        "            labels,\n",
        "            title=title + ' reconstructed digits'\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting Functions"
      ],
      "metadata": {
        "id": "egjgJtBJSBNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write fil exporting fucntions to src as export.py\n",
        "%%writefile ../CAE_setup_local/src/export.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "import subprocess\n",
        "\n",
        "def save_experiment_files(\n",
        "    experiment_name,\n",
        "    models,\n",
        "    losses,\n",
        "    reconstructions,\n",
        "    description_text,\n",
        "    local_path_root='/content'\n",
        "):\n",
        "    \"\"\"\n",
        "    Save experiment files: model weights, loss history, reconstructions, and description.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): e.g., \"experiment_2\"\n",
        "        models (list): list of trained model objects\n",
        "        losses (list): list of loss history objects\n",
        "        reconstructions (list): list of reconstructed image tensors\n",
        "        description_text (str): plain-text description\n",
        "        local_path_root (str): where to create the export folder (default: '/content')\n",
        "    \"\"\"\n",
        "\n",
        "    export_folder = os.path.join(local_path_root, f'CAE_{experiment_name}_local')\n",
        "    os.makedirs(export_folder, exist_ok=True)\n",
        "\n",
        "    for idx, (model, loss, recon) in enumerate(zip(models, losses, reconstructions)):\n",
        "        torch.save(model.state_dict(), os.path.join(export_folder, f'{experiment_name}_model_{idx+1}.pth'))\n",
        "        torch.save(loss, os.path.join(export_folder, f'{experiment_name}_loss_{idx+1}.pth'))\n",
        "        torch.save(recon, os.path.join(export_folder, f'{experiment_name}_reconstruction_{idx+1}.pth'))\n",
        "\n",
        "    with open(os.path.join(export_folder, f'{experiment_name}_description.txt'), 'w') as f:\n",
        "        f.write(description_text.strip())\n",
        "\n",
        "    print(f\"✅ Saved {experiment_name} files to: {export_folder}\")\n",
        "\n",
        "\n",
        "def export_experiment_files(experiment_name, model_count,\n",
        "                            local_root='/content',\n",
        "                            repo_root='/content/CAE-MNIST'):\n",
        "    \"\"\"\n",
        "    Copies experiment output files from local folder to Git repo and pushes them.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): e.g. \"experiment_2\"\n",
        "        model_count (int): number of models/files to export\n",
        "        local_root (str): path where local files are stored (default: /content)\n",
        "        repo_root (str): path to the cloned Git repo (default: /content/CAE-MNIST)\n",
        "    \"\"\"\n",
        "\n",
        "    # Define folders\n",
        "    local_export_folder = os.path.join(local_root, f'CAE_{experiment_name}_local')\n",
        "    git_output_folder = os.path.join(repo_root, 'outputs', f'{experiment_name}_files')\n",
        "    os.makedirs(git_output_folder, exist_ok=True)\n",
        "\n",
        "    # Gather file names\n",
        "    files_to_copy = []\n",
        "\n",
        "    # Collect filenames to copy\n",
        "    for idx in range(0, model_count):\n",
        "      files_to_copy.append(f'{experiment_name}_model_{idx+1}.pth')\n",
        "      files_to_copy.append(f'{experiment_name}_loss_{idx+1}.pth')\n",
        "      files_to_copy.append(f'{experiment_name}_reconstruction_{idx+1}.pth')\n",
        "\n",
        "    # Add description\n",
        "    files_to_copy.append(f'{experiment_name}_description.txt')\n",
        "\n",
        "    # Copy files into Git folder\n",
        "    for file in files_to_copy:\n",
        "        shutil.copy2(\n",
        "            os.path.join(local_export_folder, file),\n",
        "            os.path.join(git_output_folder, file)\n",
        "        )\n",
        "\n",
        "    # Git add, commit, push\n",
        "    os.chdir(repo_root)\n",
        "    os.system(f'git add outputs/{experiment_name}_files/*')\n",
        "    os.system(f'git commit -m \"Add {experiment_name}: models, losses, reconstructions, and description\" || echo \"Nothing to commit\"')\n",
        "    os.system('git push origin main')\n",
        "\n",
        "    print(f\"✅ Exported {experiment_name} files to: outputs/{experiment_name}_files/\")"
      ],
      "metadata": {
        "id": "r5wmvP0LOcFd",
        "outputId": "6e4582a2-2e30-4db1-c0f6-22ab4dcecedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ../CAE_setup_local/src/export.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieGQi8Vo85tn"
      },
      "source": [
        "# Push SRC to GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bnK92Lo9F0dV"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"vladislav.yushkevich.uve@gmail.com\"\n",
        "!git config --global user.name \"vlad_uve\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPxRXhhL85Dx",
        "outputId": "d1622f5c-a9c6-47c0-8d61-e7bc44802b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CAE-MNIST'...\n",
            "remote: Enumerating objects: 145, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 145 (delta 59), reused 80 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (145/145), 4.09 MiB | 25.69 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://vlad-uve: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@github.com/vlad-uve/CAE-MNIST.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ht-UrijJAO9j"
      },
      "outputs": [],
      "source": [
        "!rm -r ./CAE-MNIST/src\n",
        "!cp -r ../CAE_setup_local/src ./CAE-MNIST/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlzGCcBYA3ja",
        "outputId": "93f39405-3efa-4aa8-e416-ef4a0a0f81a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CAE-MNIST\n",
            "[main 8fe9a60] Update modularized src from CAE_setup_local\n",
            " 1 file changed, 82 insertions(+)\n",
            " create mode 100644 src/export.py\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 1.29 KiB | 1.29 MiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/vlad-uve/CAE-MNIST.git\n",
            "   6ba40ab..8fe9a60  main -> main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/CAE-MNIST\n",
        "!git add -A\n",
        "!git commit -m \"Update modularized src from CAE_setup_local\"\n",
        "!git push origin main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQbPCDH2SlJh"
      },
      "source": [
        "# Push Git Ignore to GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldjYNPw4_8Ze",
        "outputId": "ddc36037-424f-4547-d2ec-084ad955c616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/CAE-MNIST/.gitignore\n"
          ]
        }
      ],
      "source": [
        "# write gitignore\n",
        "%%writefile /content/CAE-MNIST/.gitignore\n",
        "\n",
        "%%writefile /content/CAE-MNIST/.gitignore\n",
        "# Ignore everything in outputs/ by default\n",
        "outputs/**\n",
        "\n",
        "# Allow experiment folders with valid files\n",
        "!outputs/**/\n",
        "!outputs/**/*.pth\n",
        "!outputs/**/*.pt\n",
        "!outputs/**/*.txt\n",
        "!outputs/**/*.json\n",
        "!outputs/**/*.csv\n",
        "!outputs/**/*.png\n",
        "!outputs/**/*.jpg\n",
        "\n",
        "\n",
        "# Ignore all __pycache__ and checkpoint junk\n",
        "__pycache__/\n",
        ".ipynb_checkpoints/\n",
        "*.pyc\n",
        "*.pyo\n",
        "*.pyd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce8MKkS9AI_F",
        "outputId": "cacea586-3140-48fd-b61a-d759f0ac00aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CAE-MNIST\n",
            "[main 9f31b41] Add global .gitignore for structured experiment outputs\n",
            " 1 file changed, 1 insertion(+)\n",
            "Enumerating objects: 5, done.\n",
            "Counting objects: 100% (5/5), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 341 bytes | 341.00 KiB/s, done.\n",
            "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/vlad-uve/CAE-MNIST.git\n",
            "   4d9d373..9f31b41  main -> main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/CAE-MNIST\n",
        "!git add .gitignore\n",
        "!git commit -m \"Add global .gitignore for structured experiment outputs\"\n",
        "!git push origin main"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2VXIpDzNmb8L",
        "2p8_0sl_Gt8h",
        "P3XLxXYlmZPH",
        "hWq1fORex3Cb",
        "Q1znAgpg8eaz"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}